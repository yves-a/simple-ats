# Simple ATS - Docker Compose
# Usage:
#   Development (local Ollama): docker compose --profile dev up --build
#   Production (container Ollama): docker compose --profile prod up --build

services:
  # ============================================
  # Ollama LLM Service (production only)
  # ============================================
  ollama:
    image: ollama/ollama
    container_name: ats-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - ats-network
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
    profiles:
      - prod

  # ============================================
  # Python AI Service
  # ============================================
  python-ai-dev:
    build:
      context: ./python-service
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_CACHE_DIR=/app/models
      - OLLAMA_URL=http://host.docker.internal:11434
      - ENVIRONMENT=development
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - python_models:/app/models
    networks:
      - ats-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - dev

  python-ai-prod:
    build:
      context: ./python-service
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_CACHE_DIR=/app/models
      - OLLAMA_URL=http://ollama:11434
      - ENVIRONMENT=production
    volumes:
      - python_models:/app/models
    networks:
      - ats-network
    restart: unless-stopped
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - prod

  # ============================================
  # Interview Service
  # ============================================
  interview-service-dev:
    build:
      context: .
      dockerfile: ./interview-service/Dockerfile
    ports:
      - "8001:8001"
    environment:
      - OLLAMA_URL=http://host.docker.internal:11434
      - ENVIRONMENT=development
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ats-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - dev

  interview-service-prod:
    build:
      context: .
      dockerfile: ./interview-service/Dockerfile
    ports:
      - "8001:8001"
    environment:
      - OLLAMA_URL=http://ollama:11434
      - ENVIRONMENT=production
    networks:
      - ats-network
    restart: unless-stopped
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - prod

  # ============================================
  # Java API Gateway
  # ============================================
  java-api-dev:
    build:
      context: ./java-ats
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - PYTHON_SERVICE_URL=http://python-ai-dev:8000
    depends_on:
      - python-ai-dev
    networks:
      - ats-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/ats/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - dev

  java-api-prod:
    build:
      context: ./java-ats
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - PYTHON_SERVICE_URL=http://python-ai-prod:8000
    depends_on:
      - python-ai-prod
    networks:
      - ats-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/ats/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - prod

  # ============================================
  # Frontend (Next.js)
  # ============================================
  frontend-dev:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8080
      - NODE_ENV=production
    depends_on:
      - java-api-dev
    networks:
      - ats-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - dev

  frontend-prod:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8080
      - NODE_ENV=production
    depends_on:
      - java-api-prod
    networks:
      - ats-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - prod

  # ============================================
  # Nginx Reverse Proxy (production only)
  # ============================================
  nginx:
    image: nginx:alpine
    container_name: ats-nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend-prod
      - java-api-prod
    networks:
      - ats-network
    restart: unless-stopped
    profiles:
      - prod

networks:
  ats-network:
    driver: bridge

volumes:
  ollama_models:
  python_models:
