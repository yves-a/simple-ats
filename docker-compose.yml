services:
  # Ollama LLM Service
  ollama:
    image: ollama/ollama
    container_name: ats-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - ats-network
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0

  # Python AI Service
  python-ai:
    build:
      context: ./python-service
      dockerfile: Dockerfile
    container_name: ats-python-ai
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - MODEL_CACHE_DIR=/app/models
      - OLLAMA_URL=http://ollama:11434
      - ENVIRONMENT=production  # Set to production for Docker deployment
    volumes:
      - python_models:/app/models  # Cache downloaded models
    networks:
      - ats-network
    restart: unless-stopped
    depends_on:
      - ollama

  # Java API Gateway
  java-api:
    build:
      context: ./java-ats
      dockerfile: Dockerfile
    container_name: ats-java-api
    ports:
      - "8080:8080"
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - PYTHON_SERVICE_URL=http://python-ai:8000
    depends_on:
      - python-ai
    networks:
      - ats-network
    restart: unless-stopped

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: ats-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8080
      - NODE_ENV=production
    depends_on:
      - java-api
    networks:
      - ats-network
    restart: unless-stopped

  # Nginx Reverse Proxy (Optional - for production)
  nginx:
    image: nginx:alpine
    container_name: ats-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro  # SSL certificates if needed
    depends_on:
      - frontend
      - java-api
    networks:
      - ats-network
    restart: unless-stopped
    profiles:
      - production  # Only start with --profile production

networks:
  ats-network:
    driver: bridge

volumes:
  python_models:
    driver: local
  ollama_models:
    driver: local